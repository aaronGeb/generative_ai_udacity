{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### PyTorch and HuggingFace scavenger hunt!\nPyTorch and HuggingFace have emerged as powerful tools for developing and deploying neural networks.\n\nIn this scavenger hunt, we will explore the capabilities of PyTorch and HuggingFace, uncovering hidden treasures on the way.\n\nWe have two parts:\n\n- Familiarize  with PyTorch\n- Get HuggingFace","metadata":{}},{"cell_type":"markdown","source":"### PyTorch tensors\n\nScan through the PyTorch tensors documentation [here](https://pytorch.org/docs/stable/tensors.html). Be sure to look at the examples.\n\nIn the following cell, create a tensor named `my_tensor` of size 3x3 with values of your choice. The tensor should be created on the GPU if available. Print the tensor.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer as bt\nfrom transformers import BertForSequenceClassification as bf\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:37:40.885201Z","iopub.execute_input":"2025-01-21T12:37:40.885588Z","iopub.status.idle":"2025-01-21T12:38:07.064846Z","shell.execute_reply.started":"2025-01-21T12:37:40.885556Z","shell.execute_reply":"2025-01-21T12:38:07.063734Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Neural Net Constructor Kit `torch.nn`\n\nYou can think of the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html)) module as a constructor kit for neural networks. It provides the building blocks for creating neural networks, including layers, activation functions, loss functions, and more.\n\nInstructions:\n\nCreate a three layer Multi-Layer Perceptron (MLP) neural network with the following specifications:\n\n- Input layer: 784 neurons\n- Hidden layer: 128 neurons\n- Output layer: 10 neurons\n\nUse the ReLU activation function for the hidden layer and the softmax activation function for the output layer. Print the neural network.\n\nHint: MLP's use \"fully-connected\" or \"dense\" layers. In PyTorch's `nn` module, this type of layer has a different name. See the examples in [this tutorial](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) to find out more.","metadata":{}},{"cell_type":"code","source":"class MyMLP(nn.Module):\n    \"\"\"My simple MLP model\n    Specifications:\n    - input layer: 784 neurons\n    - hidden layer: 128 neurons with ReLU activation\n    - output layer: 10 neurons with softmax activation\n    \"\"\"\n\n    def __init__(self):\n        super(MyMLP, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # pass thee input to the second layer\n        x = torch.flatten(x, start_dim=1)\n        # apply the ReLU activation function\n        x = torch.relu(self.fc1(x))\n        # pass the result to the final layer\n        x = self.fc2(x)\n        # apply the softmax activation function\n        x = self.softmax(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:38:18.184222Z","iopub.execute_input":"2025-01-21T12:38:18.184945Z","iopub.status.idle":"2025-01-21T12:38:18.191617Z","shell.execute_reply.started":"2025-01-21T12:38:18.184903Z","shell.execute_reply":"2025-01-21T12:38:18.190447Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"my_mlp = MyMLP()\nprint(my_mlp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:38:23.567926Z","iopub.execute_input":"2025-01-21T12:38:23.568280Z","iopub.status.idle":"2025-01-21T12:38:23.601684Z","shell.execute_reply.started":"2025-01-21T12:38:23.568255Z","shell.execute_reply":"2025-01-21T12:38:23.600759Z"}},"outputs":[{"name":"stdout","text":"MyMLP(\n  (fc1): Linear(in_features=784, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Check the number of inputs\nassert my_mlp.fc1.in_features == 784\n\n# Check the number of outputs\nassert my_mlp.fc2.out_features == 10\n\n# Check the number of nodes in the hidden layer\nassert my_mlp.fc1.out_features == 128\n\n# Check that my_mlp.fc1 is a fully connected layer\nassert isinstance(my_mlp.fc1, nn.Linear)\n\n# Check that my_mlp.fc2 is a fully connected layer\nassert isinstance(my_mlp.fc2, nn.Linear)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:38:27.303713Z","iopub.execute_input":"2025-01-21T12:38:27.304137Z","iopub.status.idle":"2025-01-21T12:38:27.309381Z","shell.execute_reply.started":"2025-01-21T12:38:27.304105Z","shell.execute_reply":"2025-01-21T12:38:27.308206Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### PyTorch Loss Functions and Optimizers\n\nPyTorch comes with a number of built-in loss functions and optimizers that can be used to train neural networks. The loss functions are implemented in the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)) module, while the optimizers are implemented in the `torch.optim` ([documentation](https://pytorch.org/docs/stable/optim.html)) module.\n\n\nInstructions:\n\n- Create a loss function using the `torch.nn.CrossEntropyLoss` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)) class.\n- Create an optimizer using the `torch.optim.SGD` ([documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)) class with a learning rate of 0.01.\n\n","metadata":{}},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(my_mlp.parameters(), lr=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:38:32.415792Z","iopub.execute_input":"2025-01-21T12:38:32.416171Z","iopub.status.idle":"2025-01-21T12:38:32.421338Z","shell.execute_reply.started":"2025-01-21T12:38:32.416143Z","shell.execute_reply":"2025-01-21T12:38:32.420143Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Check\n\nassert isinstance(\n    loss_function, nn.CrossEntropyLoss\n), \"loss_fn should be an instance of CrossEntropyLoss\"\nassert isinstance(optimizer, torch.optim.SGD), \"optimizer should be an instance of SGD\"\nassert optimizer.defaults[\"lr\"] == 0.01, \"learning rate should be 0.01\"\nassert optimizer.param_groups[0][\"params\"] == list(\n    my_mlp.parameters()\n), \"optimizer should be passed the MLP parameters\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:39:42.164096Z","iopub.execute_input":"2025-01-21T12:39:42.164427Z","iopub.status.idle":"2025-01-21T12:39:42.169448Z","shell.execute_reply.started":"2025-01-21T12:39:42.164403Z","shell.execute_reply":"2025-01-21T12:39:42.168348Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### PyTorch Training Loops\nPyTorch makes writing a training loop easy!\n","metadata":{}},{"cell_type":"code","source":"def fake_training_loaders():\n    for _ in range(30):\n        yield torch.randn(64, 784), torch.randint(0, 10, (64,))\n\n\nfor epoch in range(3):\n    # Create a training loop\n    for i, data in enumerate(fake_training_loaders()):\n        # Every data instance is an input + label pair\n        x, y = data\n\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Forward pass (predictions)\n        y_pred = my_mlp(x)\n\n        # Compute the loss and its gradients\n        loss = loss_function(y_pred, y)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        if i % 10 == 0:\n            print(f\"Epoch {epoch}, batch {i}: {loss.item():.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:39:48.869027Z","iopub.execute_input":"2025-01-21T12:39:48.869384Z","iopub.status.idle":"2025-01-21T12:39:49.143107Z","shell.execute_reply.started":"2025-01-21T12:39:48.869359Z","shell.execute_reply":"2025-01-21T12:39:49.141960Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, batch 0: 2.30199\nEpoch 0, batch 10: 2.30316\nEpoch 0, batch 20: 2.30135\nEpoch 1, batch 0: 2.30354\nEpoch 1, batch 10: 2.29946\nEpoch 1, batch 20: 2.30431\nEpoch 2, batch 0: 2.30483\nEpoch 2, batch 10: 2.30576\nEpoch 2, batch 20: 2.30669\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Check\n\nassert abs(loss.item() - 2.3) < 0.1, \"the loss should be around 2.3 with random data\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:39:55.580439Z","iopub.execute_input":"2025-01-21T12:39:55.580865Z","iopub.status.idle":"2025-01-21T12:39:55.585425Z","shell.execute_reply.started":"2025-01-21T12:39:55.580835Z","shell.execute_reply":"2025-01-21T12:39:55.584105Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## HuggingFace","metadata":{}},{"cell_type":"markdown","source":"### Download a model from HuggingFace and use it for sentiment analysis\n\nHuggingFace provides a number of pre-trained models that can be used for a variety of tasks. In this exercise, we will use the `distilbert-base-uncased-finetuned-sst-2-english` model to perform sentiment analysis on a movie review.\n\nInstructions:\n- Review the [AutoModel tutorial](https://huggingface.co/docs/transformers/quicktour#automodel) on the HuggingFace website.\n- Instantiate an AutoModelForSequenceClassification model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n- Instantiate an AutoTokenizer using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n- Define a function that will get a prediction","metadata":{}},{"cell_type":"code","source":" # Get the model and tokenizer\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\npt_model = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased-finetuned-sst-2-english\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n\ndef get_prediction(review):\n    \"\"\"Given a review, return the predicted sentiment\"\"\"\n\n    # Tokenize the review\n    # (Get the response as tensors and not as a list)\n    inputs = tokenizer(review, return_tensors=\"pt\")\n\n    # Perform the prediction (get the logits)\n    outputs = pt_model(**inputs)\n\n    # Get the predicted class (corresponding to the highest logit)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n\n    return \"positive\" if predictions.item() == 1 else \"negative\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:40:01.782270Z","iopub.execute_input":"2025-01-21T12:40:01.782630Z","iopub.status.idle":"2025-01-21T12:40:06.998130Z","shell.execute_reply.started":"2025-01-21T12:40:01.782602Z","shell.execute_reply":"2025-01-21T12:40:06.997154Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ad31a60e0e4af7920b87a2147ebb28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d3a8ed34754f16ae4a29389f0a90e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dab4f08bd5c4ac8a0a392365699a54f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"245f4510697646519b3bc196ced69896"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Check\n\nreview = \"This movie is not so great :(\"\n\nprint(f\"Review: {review}\")\nprint(f\"Sentiment: {get_prediction(review)}\")\n\nassert get_prediction(review) == \"negative\", \"The prediction should be negative\"\n\n\nreview = \"This movie rocks!\"\n\nprint(f\"Review: {review}\")\nprint(f\"Sentiment: {get_prediction(review)}\")\n\nassert get_prediction(review) == \"positive\", \"The prediction should be positive\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:40:09.246761Z","iopub.execute_input":"2025-01-21T12:40:09.247124Z","iopub.status.idle":"2025-01-21T12:40:09.430070Z","shell.execute_reply.started":"2025-01-21T12:40:09.247095Z","shell.execute_reply":"2025-01-21T12:40:09.428829Z"}},"outputs":[{"name":"stdout","text":"Review: This movie is not so great :(\nSentiment: negative\nReview: This movie rocks!\nSentiment: positive\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Download a dataset from HuggingFace\n\nHuggingFace provides a number of datasets that can be used for a variety of tasks. In this exercise, we will use the `imdb` dataset and pass it to the model we instantiated in the previous exercise.\n\nInstructions:\n- Review the [loading a dataset](https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html) documentation\n- Fill in the blanks","metadata":{}},{"cell_type":"code","source":"\nfrom datasets import load_dataset\n\n# Load the test split of the imdb dataset\ndataset = load_dataset(\"imdb\", split=\"test\")\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:40:16.398882Z","iopub.execute_input":"2025-01-21T12:40:16.399299Z","iopub.status.idle":"2025-01-21T12:40:27.929493Z","shell.execute_reply.started":"2025-01-21T12:40:16.399269Z","shell.execute_reply":"2025-01-21T12:40:27.928560Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f1cf6e7d7c46a2a80c344e0007af40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d22e1885a5e4a1c8d2066e64587401f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d40a0eb7f8044eba79296b70226320f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e20e339b53640858a09bd4fc68e3cf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de94290c05b842deb96d01bf8d189eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8d909b59e14a1fa77704855f6bd9d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595c01b4abd94878a983fa2e1d49cb36"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Check\n\nfrom pprint import pprint\n\nfrom datasets import Dataset\n\nassert isinstance(dataset, Dataset), \"The dataset should be a Dataset object\"\nassert set(dataset.features.keys()) == {\n    \"label\",\n    \"text\",\n}, \"The dataset should have a label and a text feature\"\n\n# Show the first example\npprint(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:40:33.333608Z","iopub.execute_input":"2025-01-21T12:40:33.334596Z","iopub.status.idle":"2025-01-21T12:40:33.352395Z","shell.execute_reply.started":"2025-01-21T12:40:33.334534Z","shell.execute_reply":"2025-01-21T12:40:33.350672Z"}},"outputs":[{"name":"stdout","text":"{'label': 0,\n 'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV '\n         'are usually underfunded, under-appreciated and misunderstood. I '\n         'tried to like this, I really did, but it is to good TV sci-fi as '\n         'Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap '\n         \"cardboard sets, stilted dialogues, CG that doesn't match the \"\n         'background, and painfully one-dimensional characters cannot be '\n         \"overcome with a 'sci-fi' setting. (I'm sure there are those of you \"\n         \"out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n         'clichéd and uninspiring.) While US viewers might like emotion and '\n         'character development, sci-fi is a genre that does not take itself '\n         'seriously (cf. Star Trek). It may treat important issues, yet not as '\n         \"a serious philosophy. It's really difficult to care about the \"\n         'characters here as they are not simply foolish, just missing a spark '\n         'of life. Their actions and reactions are wooden and predictable, '\n         \"often painful to watch. The makers of Earth KNOW it's rubbish as \"\n         'they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise '\n         \"people would not continue watching. Roddenberry's ashes must be \"\n         'turning in their orbit as this dull, cheap, poorly edited (watching '\n         'it without advert breaks really brings this home) trudging Trabant '\n         'of a show lumbers into space. Spoiler. So, kill off a main '\n         'character. And then bring him back as another actor. Jeeez! Dallas '\n         'all over again.'}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Now let's use the pre-trained model!\n\nLet's make some predictions.\n\nInstructions:\n- Fill in the blanks","metadata":{}},{"cell_type":"code","source":"# Get the last 3 reviews\nreviews = dataset[\"text\"][-3:]\n\n# Get the last 3 labels\nlabels = dataset[\"label\"][-3:]\n\n# Check\nfor review, label in zip(reviews, labels):\n    # Let's use your get_prediction function to get the sentiment\n    # of the review!\n    prediction = get_prediction(review)\n\n    print(f\"Review: {review[:80]} \\n... {review[-80:]}\")\n    print(f'Label: {\"positive\" if label else \"negative\"}')\n    print(f\"Prediction: {prediction}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:40:38.611523Z","iopub.execute_input":"2025-01-21T12:40:38.611939Z","iopub.status.idle":"2025-01-21T12:40:39.619802Z","shell.execute_reply.started":"2025-01-21T12:40:38.611912Z","shell.execute_reply":"2025-01-21T12:40:39.618697Z"}},"outputs":[{"name":"stdout","text":"Review: I got Monster Man in a box set of three films where I mainly wanted the other tw \n... ous, often gnarly splatter comedy that should endear itself to fans of the same.\nLabel: positive\nPrediction: positive\n\nReview: Five minutes in, i started to feel how naff this was looking, you've got a compl \n... for anyone who likes their horror with several side orders of gore and attitude.\nLabel: positive\nPrediction: positive\n\nReview: I caught this movie on the Sci-Fi channel recently. It actually turned out to be \n... e more than passable for the horror/slasher buff. Definitely worth checking out.\nLabel: positive\nPrediction: positive\n\n","output_type":"stream"}],"execution_count":17}]}